---
title: "[MA-589] Final Project"
author: "Wine Statisticians - Eugene Pirono, Lance, Jason Zhu"
date: "2024-04-21"
output: github_document
---


# Expectation Maximization Clustering of different Wine Types

## Introduction

Wine, a beverage celebrated for its rich diversity and intricate flavors, serves as a fascinating subject for exploratory data analysis and clustering due to the complex interplay of its attributes. Each variety of wine carries a unique signature, a profile crafted by its chemical makeup, which can be decoded using data analysis. The selection of wine as the focus of our investigation is intentional; it is a perfect blend of art and science, making it an exemplary candidate to apply and illustrate machine learning techniques. By dissecting the constituents of wine through computational lenses, we aspire to demonstrate the potent capabilities of unsupervised learning in pattern recognition and categorization.

## Project Overview

The project unfolds methodically, commencing with an exhaustive exploratory data analysis (EDA) to sift through the multitude of features inherent in wine data. This pivotal phase involves a keen examination of the relationships between various chemical properties, facilitated by statistical visualizations to discern the most informative attributes. Such scrutiny allows us to distill the essence of the dataset, cherry-picking the features that capture the essence of wine classes most effectively. Following the EDA, we embark on the preprocessing stage, standardizing the selected features to establish a level playing field, thereby priming the data for clustering. The K-Means algorithm, invoked from an R library, sets the stage as our baseline model. This widely regarded partitioning technique provides a robust starting point to gauge the clusters' cohesion and separation before delving into more sophisticated methods.

As the next step in our analytical journey, we put forth our custom implementation of the Expectation Maximization (EM) algorithm. This self-crafted algorithm, grounded in probability theory and linear algebra, serves as a testament to our understanding of the underlying statistical framework. The rigor of our EM algorithm will be evaluated against the baseline model through a suite of metrics. These comparative metrics are meticulously chosen not only to quantify the performance of each model but also to illuminate the nuances between the baseline K-Means clustering and our probabilistic EM approach. Through this meticulous comparative analysis, we aim to uncover the subtleties and strengths each method brings to the art of unsupervised learning in the context of wine categorization.

```{r cars}
df <- read.csv('data/wine_data.csv')
head(df)
```

## Exploratory Data Analysis

```{r initialize, echo=FALSE}
# Source all libraries we will be using for our analysis here:
library(readr)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(gplots)
library(reshape2)
library(stats)
library(gridExtra)
library(cluster)  # For silhouette
library(fpc)      # For Davies-Bouldin Index
library(mclust) 
library(ggrepel)    # For better label placement
library(ggforce)    # For ellipses
```

```{r Checking-Null-Observations}
sum(is.na(df))
```


```{r Scatterplot-Matrix}
cor_matrix <- cor(df[, -1]) #First column is 'class'
cor_matrix_rounded <- round(cor_matrix, 2)
corrplot(cor_matrix_rounded, method = "circle")
```

```{r Boxplots, echo=FALSE}
df_melted <- melt(df, id.vars = 'Class')

p <- ggplot(df_melted, aes(x = variable, y = value, group = interaction(variable, Class))) +
  geom_boxplot(aes(fill = as.factor(Class))) +
  facet_wrap(~variable, scales = 'free', ncol = ) +
  labs(title = "Boxplot of Features by Class", y = "Value", x = "Feature") +
  theme_minimal() +
  theme(legend.position = "bottom") 

print(p)
```

## Feature Selection

In the pursuit of effective clustering in wine datasets, the selection of variables is paramount. The aim is to choose features that are minimally correlated to each other to avoid bias in the clustering algorithm. Highly correlated variables can distort the real distribution and inter-relationships in the data, leading to misleading cluster formations. Based on an in-depth analysis of domain knowledge about wine and a detailed examination of the heatmap correlations, a strategic decision was made on which variables to include in the clustering process.

The reasoning behind the chosen variables and the decisions made are summarized in the table below:

| Considerations | Reasoning | Decision |
|----------------|-----------|----------|
| **Alcohol vs Proline** | Alcohol is pivotal in defining the flavor, preservation, and fermentation of wine. Proline, though important as an amino acid related to wine quality, shows a high correlation with Alcohol (0.64372004). | Choose **Alcohol** over Proline because it is more indicative of the wine type and is a primary characteristic. |
| **Malic Acid vs Hue** | Malic Acid plays a significant role in wine but undergoes transformations during winemaking and is strongly negatively correlated with Hue (-0.56129569), a direct indicator of wine age and quality. | Use **Hue** as it provides a direct visual representation of wine characteristics that Malic Acid does not. |
| **Total Phenols vs Flavonoids** | Total Phenols, encompassing Flavonoids, are crucial for their impact on the wine’s bitterness, astringency, and color, with a strong correlation between them (0.8645635). | Use **Total Phenols** as they cover a broader range of wine characteristics than Flavonoids alone. |
| **Color Intensity** | Color Intensity is less correlated with other features and provides insight into the wine’s age, grape type, and concentration. | Include **Color Intensity** because it captures aspects of wine not represented by the other selected features. |
| **Ash** | Ash shows moderate correlations with several attributes and may not provide unique information for clustering. | Exclude **Ash** due to its less distinctive role in differentiation. |
| **Magnesium** | Important for grapevine health and wine quality, Magnesium, however, correlates with other features and is not considered a primary characteristic for differentiation. | Exclude **Magnesium** as it does not offer primary distinguishing characteristics. |

This selection process ensures that the clustering analysis is based on features that provide unique and relevant information about the wines, potentially leading to more distinct and meaningful clusters. The choices are informed by both statistical analysis and substantive wine knowledge, reflecting a balanced approach that leverages data-driven insights and domain expertise.


```{r Feature-Selection}
df_selected_features <- df[, c('Class', 'Alcohol', 'Hue', 'TotalPhenols', 'ColorIntensity')]
head(df_selected_features)
```

```{r selected-features-boxplot}
df_melted <- melt(df_selected_features, id.vars = "Class")

p <- ggplot(df_melted, aes(x = variable, y = value, group = interaction(variable, Class))) +
  geom_boxplot(aes(fill = as.factor(Class))) +
  facet_wrap(~variable, scales = 'free', ncol = ) +
  labs(title = "Boxplot of Selected Features by Class", y = "Value", x = "Feature") +
  theme_minimal() +
  theme(legend.position = "bottom") 

print(p)

```


## Feature Standardization
Standardization is a critical preprocessing step in clustering analysis, particularly because most clustering algorithms, such as K-Means are based on measuring distances between data points. When features within a dataset vary widely in magnitudes, units, and range, algorithms that rely on Euclidean distance can be biased towards variables with larger scales. By standardizing the data (i.e., scaling each feature to have zero mean and unit variance), we ensure that each feature contributes equally to the distance computations. This prevents features with larger ranges from dominating the decision on how data points are clustered and allows the algorithm to identify more meaningful patterns in the data. Moreover, standardization can improve the convergence behavior of clustering algorithms, leading to more stable and interpretable cluster assignments.

```{r feature-standardization}
features_to_scale <- df_selected_features[, -1] 
scaled_features <- scale(features_to_scale)
df_scaled <- data.frame(Class = df_selected_features$Class, scaled_features)
summary(df_scaled)
head(df_scaled)
```


## Baseline Algorithm - K Means Clustering

```{r baseline-k-means-clustering, warning=FALSE}
features <- df_scaled[, -1]
set.seed(589) 
kmeans_result <- kmeans(features, centers = 3, nstart = 25) 

actual_labels <- df_scaled$Class
predicted_labels <- kmeans_result$cluster
ari_value <- adjustedRandIndex(actual_labels, predicted_labels)
print(paste("Adjusted Rand Index:", ari_value))

sil_widths <- silhouette(predicted_labels, dist(features))
avg_silhouette_score <- mean(sil_widths[, "sil_width"])
print(paste("Average Silhouette Score:", avg_silhouette_score))

pca_result <- prcomp(features)
df_pca <- as.data.frame(pca_result$x)
df_pca$cluster <- kmeans_result$cluster
df_pca$class <- actual_labels


ggplot(df_pca, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = as.factor(class)), size = 3, alpha = 0.6) +
  geom_point(aes(shape = as.factor(cluster)), size = 3, alpha = 0.6) +
  stat_ellipse(aes(fill = as.factor(cluster)), geom = "polygon", alpha = 0.2, show.legend = FALSE) +
  scale_color_manual(values = c("#1b9e77", "#d95f02", "#7570b3")) +
  scale_shape_manual(values = c(16, 17, 18)) +
  geom_label_repel(aes(label = ifelse(df_pca$class == df_pca$cluster, as.character(df_pca$class), paste("Class", df_pca$class, "\nCluster", df_pca$cluster))),
                   box.padding   = 0.35, 
                   point.padding = 0.5,
                   segment.color = 'grey50') +
  labs(title = 'Comparison of Actual Classes and K-Means Clusters (PCA-transformed)',
       color = "True Class",
       shape = "K-Means Cluster",
       x = 'PC1', 
       y = 'PC2') +
  theme_minimal() +
  theme(legend.position = "right", legend.title = element_blank())
```


### Adjusted Rand Index (ARI)

- **High ARI Score (0.753)**: This score suggests a strong agreement between the clustering assignments and the true classifications of the wines. The ARI, being a measure of the similarity between two data clusterings adjusted for chance, indicates that the clusters generated by the K-Means algorithm align well with the actual, underlying groupings within the wine data. This high score implies that the K-Means algorithm is effective in distinguishing between different types of wine based on their features.

### Average Silhouette Score

- **Lower Average Silhouette Score (0.383)**: While this score is not exceptionally low, it is not particularly high either, suggesting moderate separation between the clusters. In the context of wine, this score implies that while the wines are grouped into distinct categories to some extent, there remains considerable overlap or closeness between these groups. This could be due to the inherent similarities among different types of wines, where distinguishing based on the analyzed features alone does not achieve clear separation.

### Interpretation of Clustering Performance

#### Cluster Distribution and Overlap
The high ARI combined with a lower silhouette score might indicate that the true class boundaries are not perfectly separable by the spherical clusters assumed by K-Means. This situation could be common in wine data where different wine varieties (classes) may share overlapping taste profiles, chemical compositions, or production methods.

#### Cluster Density and Size Variance
The variance in cluster densities or sizes can also influence the silhouette score. For example, if a particular type of wine forms a very dense cluster while others are more spread out, the average distance within clusters compared to between clusters may decrease, leading to a lower silhouette score.

#### Noise and Outliers
Noise and outliers in wine data, such as unusual wines or misrecorded data points, can disproportionately affect the silhouette score more than the ARI. This could skew the average distance calculations, impacting the clarity of cluster boundaries.


## EM Algorithm - Gaussian Mixture Model

```{r EM_GMM_MODEL}



```


